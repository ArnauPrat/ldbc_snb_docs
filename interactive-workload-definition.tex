The \emph{Test Driver} is in charge of the execution of the Interactive Workload.
At the beginning of the execution, the Test Driver creates a query mix by
assigning to each query instance, a query issue time and a set of parameters
taken from the generated substitution parameter set described above.  

Query issue times have to be carefully assigned.  Although substitution
parameters are chosen in such a way that queries of the same type take similar
time, not all query types have the same complexity and touch the same amount of
data, which causes them to scale differently for the different scale factors.
Therefore, if all query instances, regardless of their type, are issued
at the same rate, those more complex queries will dominate the execution's
result, making faster query types purposeless. To avoid this situation, each
query type is executed at a different rate. The way the execution rate is decided,
also depends on the nature of the query: complex read, short read or update.

Update queries' issue times are taken from the update streams generated by the
data generator. These are the times where the actual event happened during the
simulation of the social network. Complex reads' times are expressed in terms
of update operations. For each complex read query type, a frequency value is
assigned which specifies the relation between the number of updates performed
per complex read. \autoref{table:freqs} shows the frequencies for each complex query and SF used in the Interactive workload (\autoref{sec:interactive}).

%\todo{Should we define frequencies for SF0.1 and SF0.3? No, those are for testing, not benchmarking.}

\input{tables/table-interactive-frequencies}

Finally, short reads are inserted in order to balance the ratio between reads
and writes, and to simulate the behavior of a real user of the social network.
For each complex read instance, a sequence of short reads is planned. There are two
types of short read sequences: Person centric and Message centric. Depending on
the type of the complex read, one of them is chosen. Each sequence consists of
a set of short reads which are issued in a row. The issue time assigned to each
short read in the sequence is determined at run time, and is based on the
completion time of the complex read it depends on. 
The substitution parameters for short reads are taken from the results of previously
executed queries.

\begin{itemize}
\item Complex reads:
    \queryRefCard{interactive-complex-read-01}{IC}{1}
    \queryRefCard{interactive-complex-read-02}{IC}{2}
    \queryRefCard{interactive-complex-read-03}{IC}{3}
    \queryRefCard{interactive-complex-read-07}{IC}{7}
    \queryRefCard{interactive-complex-read-08}{IC}{8}
    \queryRefCard{interactive-complex-read-09}{IC}{9}
    \queryRefCard{interactive-complex-read-10}{IC}{10}
    \queryRefCard{interactive-complex-read-11}{IC}{11}
    \queryRefCard{interactive-complex-read-12}{IC}{12}
    \queryRefCard{interactive-complex-read-14-old}{IC}{14-o}
    \queryRefCard{interactive-complex-read-14-new}{IC}{14-n}
\item Short reads:
    \queryRefCard{interactive-short-read-02}{IS}{2}
    \queryRefCard{interactive-short-read-03}{IS}{3}
    \queryRefCard{interactive-short-read-05}{IS}{5}
    \queryRefCard{interactive-short-read-06}{IS}{6}
    \queryRefCard{interactive-short-read-07}{IS}{7}
\end{itemize}

For the precise connections between short and complex read queries, see \autoref{tab:short-read-after-long-read}.

Once a short read sequence is issued (and provided that sufficient substitution parameters 
exist), there is a probability that another short read  sequence is issued. 
This probability decreases for each new sequence issued. 
Since the same random number generator seed is used across
executions, the workload is deterministic.

\input{tables/table-short-read-queries}

The specified frequencies, implicitly define the query ratios between queries
of different types, as well as a default target throughput. However, the Test
Sponsor may specify a different target throughput to test,  by ``squeezing''
together or ``stretching'' apart the queries of the workload. This is
achieved by means of the ``Time Compression Ratio'' that is multiplied by the
frequencies (see \autoref{table:freqs}).  Therefore, different
throughputs can be tested while maintaining the relative ratios between the
different query types.
